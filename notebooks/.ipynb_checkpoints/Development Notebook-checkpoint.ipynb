{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e837d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycss.utils import *\n",
    "from pycss.subset_selection import *\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "a41433fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOL =1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "a18f68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def css_score(Sigma_R, tol=TOL):\n",
    "    diag = np.diag(Sigma_R)\n",
    "    return -1 * np.divide(np.sum(np.square(Sigma_R), axis=1), diag, out=np.zeros_like(diag, dtype=float), where=(diag > tol))\n",
    "\n",
    "def check_greedy_css_inputs(Sigma, k, cutoffs, include, exclude, tol):\n",
    "    n, p = Sigma.shape \n",
    "\n",
    "    if not n == p:\n",
    "        raise ValueError(\"Sigma must be a square matrix.\")\n",
    "  \n",
    "    if k is None and cutoffs is None:\n",
    "        raise ValueError(\"One of k or cutoff must not be None.\")\n",
    "  \n",
    "    if k is not None and cutoffs is not None:\n",
    "        raise ValueError(\"Only one of k or cutoff can be None.\")\n",
    "\n",
    "    if cutoffs is not None:\n",
    "        if (isinstance(cutoffs, (list, np.ndarray)) and not len(cutoffs) == p) or not isinstance(cutoffs, (int, np.integer, float)):\n",
    "            raise ValueError(\"Cutoffs must be a single value or length p.\")\n",
    "\n",
    "    if k is not None and not isinstance(k, (int, np.integer)):\n",
    "        raise ValueError(\"k must be an integer.\")\n",
    "    if k is not None and (k <= 0 or k > p):\n",
    "        raise ValueError(\"k must be > 0 and <= p.\")\n",
    "\n",
    "    if not set(exclude).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Exclude must be a subset of the available indices.\")\n",
    "    if not set(include).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Include must be a subset of the available indices.\")\n",
    "    if len(exclude) == p:\n",
    "        raise ValueError(\"Cannot exclude everything.\")\n",
    "    if k is not None and len(include) > k:\n",
    "        raise ValueError(\"Cannot include more than k.\")\n",
    "    if k is not None and len(exclude) > p - k:\n",
    "        raise ValueError(\"Cannot exclude more than p-k.\")\n",
    "\n",
    "\n",
    "    if not isinstance(tol, float):\n",
    "        raise ValueError(\"tol must be a float.\")\n",
    "\n",
    "    return\n",
    "\n",
    "def greedy_css(Sigma,\n",
    "               k=None,\n",
    "               cutoffs=None,\n",
    "               include=np.array([]),\n",
    "               exclude=np.array([]),\n",
    "               tol=TOL,\n",
    "               ):\n",
    "\n",
    "    check_greedy_css_inputs(Sigma, k, cutoffs, include, exclude, tol)\n",
    "\n",
    "    Sigma_R = Sigma.copy()\n",
    "    p = Sigma.shape[0]\n",
    "    S = -1 * np.ones(p).astype(int)\n",
    "\n",
    "    if isinstance(cutoffs, (int, np.integer, float)):\n",
    "        cutoffs = cutoffs * np.ones(p)\n",
    "\n",
    "    idx_order = np.arange(p)\n",
    "    num_active = p\n",
    "\n",
    "    selected_enough = False\n",
    "    num_selected = 0\n",
    "\n",
    "    while not selected_enough:\n",
    "\n",
    "        # subset to acvice variables\n",
    "        Sigma_R_active = Sigma_R[:num_active, :num_active]\n",
    "\n",
    "        if num_selected < len(include):\n",
    "            j_star = np.where(idx_order == include[num_selected])[0][0]\n",
    "\n",
    "            # If the include variables are colinear return a colinearity error\n",
    "            if j_star > num_active - 1:\n",
    "                warnings.warn(\"Variables \" + str(include[:num_selected + 1]) + \" that have been requested to be included are colinear.\")\n",
    "                S[num_selected] = idx_order[j_star]\n",
    "                num_selected += 1\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            # compute objective values\n",
    "            obj_vals = css_score(Sigma_R_active, tol=tol)\n",
    "\n",
    "            # set the exclude objective values to infinity\n",
    "            obj_vals[np.in1d(idx_order[:num_active], exclude)] = np.inf\n",
    "            # select next variable\n",
    "            j_star = random_argmin(obj_vals)\n",
    "\n",
    "        S[num_selected] = idx_order[j_star]\n",
    "        num_selected += 1\n",
    "\n",
    "        # regress off selected variable\n",
    "        regress_one_off_in_place(Sigma_R_active, j_star, tol=tol)\n",
    "\n",
    "        # swap selected variable with last active position\n",
    "        swap_in_place(Sigma_R, [j_star], [num_active - 1], idx_order=idx_order)\n",
    "        # decrement number active\n",
    "        num_active -= 1\n",
    "\n",
    "        # swap any variables with < tol variance to bottom and update num active\n",
    "        zero_idxs = np.where(np.diag(Sigma_R_active)[:num_active] < tol)[0]\n",
    "        num_zero_idxs = len(zero_idxs)\n",
    "        idxs_to_swap = np.arange(num_active - num_zero_idxs, num_active)\n",
    "        swap_in_place(Sigma_R, zero_idxs, idxs_to_swap, idx_order=idx_order)\n",
    "        num_active -= num_zero_idxs\n",
    "\n",
    "        # continue if not enough included\n",
    "        if num_selected < len(include):\n",
    "            continue\n",
    "        # terminate early if all variables are explained\n",
    "        if set(idx_order[:num_active]).issubset(exclude):\n",
    "            selected_enough = True\n",
    "        # terminate if user requested k and k have been selected\n",
    "        if k is not None and num_selected == k:\n",
    "            selected_enough = True\n",
    "        # terminate if below user's cutoff\n",
    "        if cutoffs is not None and np.trace(Sigma_R) <= cutoffs[num_selected - 1]:\n",
    "            selected_enough = True\n",
    "\n",
    "    perm_in_place(Sigma_R, np.arange(p), np.argsort(idx_order))\n",
    "\n",
    "    return S[:num_selected], Sigma_R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "efb78979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_factor_score(Sigma_R, tol=TOL):\n",
    "\n",
    "    diag = np.diag(Sigma_R)\n",
    "    resids = diag - (1/diag)[:, None] * np.square(Sigma_R)\n",
    "    np.fill_diagonal(resids, 1)\n",
    "\n",
    "    if np.any(resids < tol):\n",
    "        return None, np.where(resids < tol)\n",
    "    else:\n",
    "        objective_values = np.log(diag) + np.sum(np.log(resids), axis=1)\n",
    "        return objective_values, (np.array([]), np.array([]))\n",
    "\n",
    "def check_greedy_subset_factor_inputs(Sigma, cutoffs, include, exclude, tol):\n",
    "    n, p = Sigma.shape \n",
    "\n",
    "    if not n == p:\n",
    "        raise ValueError(\"Sigma must be a square matrix.\")\n",
    "  \n",
    "    if not isinstance(cutoffs, (list, np.ndarray)) or not len(cutoffs) == p + 1:\n",
    "        raise ValueError(\"Must provide p + 1 cutoffs.\")\n",
    "\n",
    "    if not set(exclude).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Exclude must be a subset of the available indices.\")\n",
    "    if not set(include).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Include must be a subset of the available indices.\")\n",
    "\n",
    "    if not isinstance(tol, float):\n",
    "        raise ValueError(\"tol must be a float.\")\n",
    "\n",
    "    return\n",
    "\n",
    "def greedy_subset_factor_selection(Sigma,\n",
    "                                   cutoffs,\n",
    "                                   include=np.array([]),\n",
    "                                   exclude=np.array([]),\n",
    "                                   tol=TOL,\n",
    "                                   ):\n",
    "\n",
    "    check_greedy_subset_factor_inputs(Sigma, cutoffs, include, exclude, tol)\n",
    "    \n",
    "    Sigma_R = Sigma.copy()\n",
    "    p = Sigma.shape[0]\n",
    "    \n",
    "    # check if size-0 subset is sufficient \n",
    "    reject = np.sum(np.log(np.diag(Sigma_R))) > cutoffs[0]\n",
    "    if (not reject and len(include) == 0) or len(exclude) == p:\n",
    "        return np.array([]), reject\n",
    "        \n",
    "    \n",
    "    S = -1 * np.ones(p).astype(int)\n",
    "    idx_order = np.arange(p)\n",
    "    num_active = p\n",
    "\n",
    "    selected_enough = False\n",
    "    num_selected = 0\n",
    "\n",
    "    running_residuals = -1 * np.ones(p)\n",
    "\n",
    "    # subset to acvice variables\n",
    "    Sigma_R_active = Sigma_R[:num_active, :num_active]\n",
    "    while not selected_enough:\n",
    "\n",
    "        if num_selected < len(include):\n",
    "            j_star = np.where(idx_order == include[num_selected])[0][0]\n",
    "        else:\n",
    "            # compute objective values\n",
    "            obj_vals, colinearity_errors = subset_factor_score(Sigma_R_active, tol=tol)\n",
    "\n",
    "            # if adding a variable results in zero residual, warn the user and fail to reject\n",
    "            if len(colinearity_errors[0]) > 0:\n",
    "                warnings.warn(\"When you add variable \"  + str(colinearity_errors[0]) + \" to \" + str(S[:num_selected]) + \" it perfectly explains variable \" + str(colinearity_errors[1]) + \".\")\n",
    "                reject = False\n",
    "                return np.concatenate([S[:num_selected], np.array(colinearity_errors[0][1])]), reject\n",
    "\n",
    "            # set the exclude objective values to infinity\n",
    "            obj_vals[np.in1d(idx_order[:num_active], exclude)] = np.inf\n",
    "            # select next variable\n",
    "            j_star = random_argmin(obj_vals)\n",
    "\n",
    "        S[num_selected] = idx_order[j_star]\n",
    "        running_residuals[num_selected] = Sigma_R_active[j_star, j_star]\n",
    "        num_selected += 1\n",
    "\n",
    "        # regress off selected variable\n",
    "        regress_one_off_in_place(Sigma_R_active, j_star, tol=tol)\n",
    "\n",
    "        # swap selected variable with last active position\n",
    "        swap_in_place(Sigma_R, [j_star], [num_active - 1], idx_order=idx_order)\n",
    "        # decrement number active\n",
    "        num_active -= 1\n",
    "\n",
    "        # subset_to_active_variables\n",
    "        Sigma_R_active = Sigma_R[:num_active, :num_active]\n",
    "\n",
    "        # when including variables that have been requested to be included, ensure that no residuals are zero\n",
    "        if num_selected <= len(include):\n",
    "            zeros = np.where(np.diag(Sigma_R_active) < tol)[0]\n",
    "            if len(zeros) > 0:\n",
    "                warnings.warn(\"Variables \" + str(S[:num_selected]) + \" perfectly explain \" + str(idx_order[zeros]) + \".\")\n",
    "                reject = False\n",
    "                return include, reject\n",
    "\n",
    "        # continue if not enough included\n",
    "        if num_selected < len(include):\n",
    "            continue\n",
    "\n",
    "        # if we fail to reject, terminate and return\n",
    "        if np.sum(np.log(np.diag(Sigma_R_active))) + np.sum(np.log(running_residuals[:num_selected])) <= cutoffs[num_selected]:\n",
    "            reject = False\n",
    "            selected_enough = True\n",
    "\n",
    "        # forcefully fail to reject once we've selected at least p-1 in case of numerical instability\n",
    "        if num_selected >= p - 1:\n",
    "            reject = False\n",
    "            selected_enough = True\n",
    "\n",
    "        # terminate if no variables are left to select\n",
    "        if set(idx_order[:num_active]).issubset(exclude):\n",
    "            selected_enough = True\n",
    "\n",
    "    return S[:num_selected], reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "348418aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_swapping_css_inputs(Sigma,\n",
    "                              k,\n",
    "                              max_iter,\n",
    "                              num_inits, \n",
    "                              S_init,\n",
    "                              include,\n",
    "                              exclude,\n",
    "                              tol):\n",
    "    n, p = Sigma.shape \n",
    "\n",
    "    if not n == p:\n",
    "        raise ValueError(\"Sigma must be a square matrix.\")\n",
    "\n",
    "    if not isinstance(k, (int, np.integer)) or k <= 0 or k > p:\n",
    "        raise ValueError(\"k must be an integer > 0 and <= p.\")\n",
    "    \n",
    "    if S_init is not None:\n",
    "        if not isinstance(S_init, np.ndarray) or  len(set(S_init)) != k or (not set(S_init).issubset(np.arange(p))):\n",
    "            raise ValueError(\"S_init must be a numpy array of length k containing indices 0 to p-1.\")\n",
    "        if not set(include).issubset(S_init):\n",
    "            raise ValueError(\"Include must be a subset of S_init.\")\n",
    "        if len(set(exclude).intersection(S_init)) > 0:\n",
    "            raise ValueError(\"S_init cannot contain any elements in exlcude.\")\n",
    "        \n",
    "    if not set(exclude).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Exclude must be a subset of the available indices.\")\n",
    "    if not set(include).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Include must be a subset of the available indices.\")\n",
    "\n",
    "    if len(include) > k:\n",
    "        raise ValueError(\"Cannot include more than k.\")\n",
    "    if k is not None and len(exclude) > p - k:\n",
    "        raise ValueError(\"Cannot exclude more than p-k.\")\n",
    "\n",
    "\n",
    "    if not isinstance(tol, float):\n",
    "        raise ValueError(\"tol must be a float.\")\n",
    "\n",
    "def swapping_css_with_init(Sigma,\n",
    "                           S_init,\n",
    "                           max_iter,\n",
    "                           include,\n",
    "                           exclude,\n",
    "                           tol=TOL):\n",
    "    k = len(S_init)\n",
    "    p = Sigma.shape[0]\n",
    "    d = p-k\n",
    "    include_set = set(include)\n",
    "\n",
    "    idx_order = np.arange(p)\n",
    "\n",
    "    Sigma_R = Sigma.copy()\n",
    "    # these will always be the indices of the selected subset\n",
    "    subset_idxs = np.arange(d, p)\n",
    "    # swap initial variables to bottom of Sigma\n",
    "    swap_in_place(Sigma_R, subset_idxs, S_init, idx_order=idx_order)\n",
    "    S = idx_order[d:].copy()\n",
    "    Sigma_S = Sigma[:, S][S, :].copy()\n",
    "    invertible, Sigma_S_L = is_invertible(Sigma_S)   \n",
    "\n",
    "    if not invertible:\n",
    "        return None, None, None \n",
    "\n",
    "    regress_off_in_place(Sigma_R, np.arange(d, p))\n",
    "\n",
    "    # number of completed iterations\n",
    "    N = 0\n",
    "    # counter of how many consecutive times we have chose not to swap \n",
    "    not_replaced = 0\n",
    "    # permutation which shifts the last variable in the subset to the top of the subset\n",
    "    subset_idxs_permuted = np.concatenate([subset_idxs[1:], np.array([subset_idxs[0]])])\n",
    "    converged = False\n",
    "\n",
    "    while N < max_iter and (not converged):\n",
    "        for i in range(k):\n",
    "            S_0 = S[0]\n",
    "\n",
    "            # Update cholesky after removing first variable from subset\n",
    "            Sigma_T_L = update_cholesky_after_removing_first(Sigma_S_L)\n",
    "\n",
    "            if S_0 not in include_set:\n",
    "            \n",
    "                # Subest with first variable removed  from selected subset\n",
    "                T = S[1:]\n",
    "\n",
    "                # Update residual covariance after removing first variable from subset\n",
    "                v = Sigma[:, S_0] - Sigma[:, T] @ solve_with_cholesky(Sigma_T_L, Sigma[T, S_0]) if k > 1 else Sigma[:, S_0]\n",
    "                reordered_v = v[idx_order]\n",
    "                Sigma_R = Sigma_R + np.outer(reordered_v, reordered_v)/v[S_0]\n",
    "                \n",
    "                # Swap first variable from subset to to top of residual matrix\n",
    "                swap_in_place(Sigma_R, np.array([0]), np.array([d]), idx_order=idx_order)\n",
    "\n",
    "                # find indices of variables with zero variance\n",
    "                zero_idxs = np.where(np.diag(Sigma_R)[:(d + 1)] <= tol)[0]\n",
    "                num_zero_idxs = len(zero_idxs)\n",
    "                # In residual matrix, swap variables with zero indices to right above currently selected subset (of size k-1)\n",
    "                swap_in_place(Sigma_R, zero_idxs, np.arange(d + 1 - num_zero_idxs, d + 1), idx_order=idx_order)\n",
    "                \n",
    "                # update num_active\n",
    "                num_active = d + 1 - num_zero_idxs\n",
    "\n",
    "                # compute objectives and for active variables and find minimizers\n",
    "                obj_vals = css_score(Sigma_R[:num_active, :num_active], tol=tol)\n",
    "\n",
    "                # set the objective value to infinity for the excluded variables\n",
    "                obj_vals[np.in1d(idx_order[:num_active], exclude)] = np.inf\n",
    "\n",
    "                choices = np.flatnonzero(obj_vals == obj_vals.min())\n",
    "\n",
    "                # if removed variable is a choice, select it, otherwise select a random choice\n",
    "                if 0 in choices:\n",
    "                    not_replaced += 1\n",
    "                    j_star = 0\n",
    "                else:\n",
    "                    not_replaced = 0\n",
    "                    j_star = np.random.choice(choices)\n",
    "                \n",
    "                S_new = idx_order[j_star]\n",
    "                \n",
    "                # In residual covariance, regress selected variable off the remaining\n",
    "                #regress_one_off_in_place(Sigma_R[:(d+1), :(d+1)], j_star) #alternative option\n",
    "                regress_one_off_in_place(Sigma_R[:num_active, :num_active], j_star)\n",
    "                # In residual covariance swap new choice to top of selected subset \n",
    "                swap_in_place(Sigma_R, np.array([j_star]), np.array([d]), idx_order=idx_order)\n",
    "              \n",
    "            else:\n",
    "                S_new = S_0 \n",
    "            \n",
    "            # Add new choice as the last variable in selected subset\n",
    "            S[:k-1] = S[1:]\n",
    "            S[k-1] = S_new\n",
    "            # Update cholesky after adding new choice as last variable in selected subset\n",
    "            Sigma_S_L = update_cholesky_after_adding_last(Sigma_T_L, Sigma[S_new, S])\n",
    "            \n",
    "            # permute first variables in selected subset to the last variable in the residual matrix\n",
    "            perm_in_place(Sigma_R, subset_idxs,  subset_idxs_permuted, idx_order=idx_order)\n",
    "\n",
    "            if not_replaced == k - len(include):\n",
    "                converged=True\n",
    "                break\n",
    "\n",
    "        N += 1\n",
    "\n",
    "    perm_in_place(Sigma_R, np.arange(p), np.argsort(idx_order))\n",
    "    \n",
    "    return S, Sigma_R, converged \n",
    "\n",
    "def swapping_css(Sigma,\n",
    "                 k,\n",
    "                 max_iter=100,\n",
    "                 num_inits=1, \n",
    "                 S_init=None,\n",
    "                 include=np.array([]),\n",
    "                 exclude=np.array([]),\n",
    "                 tol=TOL):\n",
    "\n",
    "    check_swapping_css_inputs(Sigma,\n",
    "                              k,\n",
    "                              max_iter,\n",
    "                              num_inits, \n",
    "                              S_init,\n",
    "                              include,\n",
    "                              exclude,\n",
    "                              tol)\n",
    "    \n",
    "    best_converged = None\n",
    "    best_S = None\n",
    "    best_S_init = None \n",
    "    best_Sigma_R = None\n",
    "    best_obj_val = np.inf \n",
    "    not_include = np.array([idx for idx in complement(Sigma.shape[0], include) if idx not in set(exclude)])\n",
    "    \n",
    "    if len(include) > 0:\n",
    "        invertible, _ = is_invertible(Sigma[include, :][:, include], tol=tol)\n",
    "        if not invertible:\n",
    "            warnings.warn(\"The variables requested to be included are colinear.\")\n",
    "            return best_S, best_Sigma_R, best_S_init, best_converged   \n",
    "    \n",
    "    no_initialization = (S_init is None)\n",
    "    if not no_initialization:\n",
    "        num_inits = 1\n",
    "\n",
    "    for _ in range(num_inits):\n",
    "        if no_initialization:\n",
    "            S_init = np.concatenate([include, np.random.choice(not_include, k-len(include), replace=False)]).astype(int)\n",
    "\n",
    "        S, Sigma_R, converged  = swapping_css_with_init(Sigma=Sigma,\n",
    "                                                        S_init=S_init,\n",
    "                                                        max_iter=max_iter, \n",
    "                                                        include=include,\n",
    "                                                        exclude=exclude,\n",
    "                                                        tol=TOL)\n",
    "        if S is None:\n",
    "            continue \n",
    "      \n",
    "        obj_val = np.trace(Sigma_R)\n",
    "        if obj_val < best_obj_val:\n",
    "            best_obj_val = obj_val \n",
    "            best_S = S\n",
    "            best_S_init = S_init\n",
    "            best_Sigma_R = Sigma_R\n",
    "            best_converged = converged \n",
    "\n",
    "    if best_S is None:\n",
    "        warnings.warn(\"All the initializations tried were colinear.\")\n",
    "    return best_S, best_Sigma_R, best_S_init, best_converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "e7600fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_swapping_subest_factor_inputs(Sigma,\n",
    "                                        k,\n",
    "                                        cutoff, \n",
    "                                        max_iter,\n",
    "                                        num_inits, \n",
    "                                        S_init,\n",
    "                                        find_minimizer,\n",
    "                                        include,\n",
    "                                        exclude,\n",
    "                                        tol):\n",
    "    n, p = Sigma.shape \n",
    "\n",
    "    if not n == p:\n",
    "        raise ValueError(\"Sigma must be a square matrix.\")\n",
    "\n",
    "    if not isinstance(k, (int, np.integer)) or k < 0 or k > p:\n",
    "        raise ValueError(\"k must be an integer between 0 and p (inclusive).\")\n",
    "    \n",
    "    if S_init is not None:\n",
    "        if not isinstance(S_init, np.ndarray) or  len(set(S_init)) != k or (not set(S_init).issubset(np.arange(p))):\n",
    "            raise ValueError(\"S_init must be a numpy array of length k containing indices 0 to p-1.\")\n",
    "        if not set(include).issubset(S_init):\n",
    "            raise ValueError(\"Include must be a subset of S_init.\")\n",
    "        if len(set(exclude).intersection(S_init)) > 0:\n",
    "            raise ValueError(\"S_init cannot contain any elements in exlcude.\")\n",
    "        \n",
    "    if not set(exclude).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Exclude must be a subset of the available indices.\")\n",
    "    if not set(include).issubset(np.arange(p)):\n",
    "        raise ValueError(\"Include must be a subset of the available indices.\")\n",
    "\n",
    "    if len(include) > k:\n",
    "        raise ValueError(\"Cannot include more than k.\")\n",
    "    if k is not None and len(exclude) > p - k:\n",
    "        raise ValueError(\"Cannot exclude more than p-k.\")\n",
    "\n",
    "\n",
    "    if not isinstance(tol, float):\n",
    "        raise ValueError(\"tol must be a float.\")\n",
    "    \n",
    "    return \n",
    "\n",
    "def swapping_subset_factor_with_init(Sigma, \n",
    "                                     S_init,\n",
    "                                     find_minimizer,\n",
    "                                     cutoff, \n",
    "                                     max_iter,\n",
    "                                     include,\n",
    "                                     exclude,\n",
    "                                     tol=TOL):\n",
    "    \n",
    "    \n",
    "    k = len(S_init)\n",
    "    \n",
    "    # handle case where subset must be empty \n",
    "    if k == 0:\n",
    "        log_det = np.sum(np.log(np.diag(Sigma)))\n",
    "        reject = log_det > cutoff\n",
    "        return np.array([]), reject, log_det \n",
    "    \n",
    "    p = Sigma.shape[0]\n",
    "    d = p-k\n",
    "    include_set = set(include)\n",
    "\n",
    "    idx_order = np.arange(p)\n",
    "\n",
    "    Sigma_R = Sigma.copy()\n",
    "    # these will always be the indices of the selected subset\n",
    "    subset_idxs = np.arange(d, p)\n",
    "    # swap initial variables to bottom of Sigma\n",
    "    swap_in_place(Sigma_R, subset_idxs, S_init, idx_order=idx_order)\n",
    "    S = idx_order[d:].copy()\n",
    "    Sigma_S = Sigma[:, S][S, :].copy()\n",
    "    invertible, Sigma_S_L = is_invertible(Sigma_S)   \n",
    "\n",
    "    if not invertible:\n",
    "        warnings.warn(\"Variables \" + str(S_init) + \" are colinear.\" )\n",
    "        reject = False\n",
    "        return S_init, reject, -np.inf  \n",
    "\n",
    "    regress_off_in_place(Sigma_R, np.arange(d, p))\n",
    "    \n",
    "    where_zeros = np.where(np.diag(Sigma_R)[:d] < tol)[0]\n",
    "    if len(where_zeros > 0):\n",
    "        warnings.warn(\"Variables \" + str(S_init) + \" perfectly explain \" + str(idx_order[where_zeros]) )\n",
    "        reject = False \n",
    "        return S_init, reject, -np.inf \n",
    "    \n",
    "\n",
    "    # number of completed iterations\n",
    "    N = 0\n",
    "    # counter of how many consecutive times we have chose not to swap \n",
    "    not_replaced = 0\n",
    "    # permutation which shifts the last variable in the subset to the top of the subset\n",
    "    subset_idxs_permuted = np.concatenate([subset_idxs[1:], np.array([subset_idxs[0]])])\n",
    "    converged = False\n",
    "\n",
    "    while N < max_iter and (not converged):\n",
    "        for i in range(k):\n",
    "            S_0 = S[0]\n",
    "\n",
    "            # Update cholesky after removing first variable from subset\n",
    "            Sigma_T_L = update_cholesky_after_removing_first(Sigma_S_L)\n",
    "\n",
    "            if S_0 not in include_set:\n",
    "            \n",
    "                # Subest with first variable removed from selected subset\n",
    "                T = S[1:]\n",
    "\n",
    "                # Update residual covariance after removing first variable from subset\n",
    "                v = Sigma[:, S_0] - Sigma[:, T] @ solve_with_cholesky(Sigma_T_L, Sigma[T, S_0]) if k > 1 else Sigma[:, S_0]\n",
    "                reordered_v = v[idx_order]\n",
    "                Sigma_R = Sigma_R + np.outer(reordered_v, reordered_v)/v[S_0]\n",
    "                \n",
    "                # Swap first variable from subset to to top of residual matrix\n",
    "                swap_in_place(Sigma_R, np.array([0]), np.array([d]), idx_order=idx_order)\n",
    "                \n",
    "                # compute objectives and for active variables and find minimizers\n",
    "                obj_vals, colinearity_errors = subset_factor_score(Sigma_R[:(d+1), :(d+1)], tol=tol)\n",
    "\n",
    "                # if adding a variable results in zero residual, warn the user and fail to reject\n",
    "                if len(colinearity_errors[0]) > 0:\n",
    "                    warnings.warn(\"When you add variable \"  + str(colinearity_errors[0]) + \" to \" + str(S[:num_selected]) + \" it perfectly explains variable \" + str(colinearity_errors[1]) + \".\")\n",
    "                    reject = False\n",
    "                    return np.concatenate([T, np.array(colinearity_errors[0][1])]), reject, -np.inf\n",
    "\n",
    "                # set the objective value to infinity for the excluded variables\n",
    "                obj_vals[np.in1d(idx_order[:(d+1)], exclude)] = np.inf\n",
    "\n",
    "                choices = np.flatnonzero(obj_vals == obj_vals.min())\n",
    "\n",
    "                # if removed variable is a choice, select it, otherwise select a random choice\n",
    "                if 0 in choices:\n",
    "                    not_replaced += 1\n",
    "                    j_star = 0\n",
    "                else:\n",
    "                    not_replaced = 0\n",
    "                    j_star = np.random.choice(choices)\n",
    "                \n",
    "                S_new = idx_order[j_star]\n",
    "                \n",
    "                # In residual covariance, regress selected variable off the remaining\n",
    "                regress_one_off_in_place(Sigma_R[:(d+1), :(d+1)], j_star)\n",
    "                # In residual covariance swap new choice to top of selected subset \n",
    "                swap_in_place(Sigma_R, np.array([j_star]), np.array([d]), idx_order=idx_order)\n",
    "              \n",
    "            else:\n",
    "                S_new = S_0 \n",
    "            \n",
    "            # Add new choice as the last variable in selected subset\n",
    "            S[:k-1] = S[1:]\n",
    "            S[k-1] = S_new\n",
    "            # Update cholesky after adding new choice as last variable in selected subset\n",
    "            Sigma_S_L = update_cholesky_after_adding_last(Sigma_T_L, Sigma[S_new, S])\n",
    "            \n",
    "            # permute first variables in selected subset to the last variable in the residual matrix\n",
    "            perm_in_place(Sigma_R, subset_idxs,  subset_idxs_permuted, idx_order=idx_order)\n",
    "            \n",
    "            # If you don't want to find the minimizer and log det is small enough, terminate now\n",
    "            if not find_minimizer:\n",
    "                log_det = np.sum(np.log(np.diag(Sigma_R)[:d])) + np.sum(np.log(np.square(np.diag(Sigma_S_L))))\n",
    "                if log_det <= cutoff:\n",
    "                    reject = False\n",
    "                    return S, reject, log_det\n",
    "\n",
    "            if not_replaced == k - len(include):\n",
    "                converged=True\n",
    "                break\n",
    "\n",
    "        N += 1\n",
    "\n",
    "    log_det = np.sum(np.log(np.diag(Sigma_R)[:d])) + np.sum(np.log(np.square(np.diag(Sigma_S_L))))\n",
    "    reject = (log_det > cutoff)\n",
    "    return S, reject, log_det\n",
    "\n",
    "def swapping_subset_factor_selection(Sigma,\n",
    "                                     k,\n",
    "                                     cutoff,\n",
    "                                     max_iter=100,\n",
    "                                     num_inits=1, \n",
    "                                     S_init=None,\n",
    "                                     find_minimizer=False, \n",
    "                                     include=np.array([]),\n",
    "                                     exclude=np.array([]),\n",
    "                                     tol=TOL):\n",
    "    \n",
    "    check_swapping_subest_factor_inputs(Sigma,\n",
    "                                        k,\n",
    "                                        cutoff, \n",
    "                                        max_iter,\n",
    "                                        num_inits, \n",
    "                                        S_init,\n",
    "                                        find_minimizer,\n",
    "                                        include,\n",
    "                                        exclude,\n",
    "                                        tol)\n",
    "    \n",
    "    reject = True\n",
    "    best_S = None\n",
    "    best_log_det = np.inf \n",
    "    not_include = np.array([idx for idx in complement(Sigma.shape[0], include) if idx not in set(exclude)])\n",
    "    \n",
    "    if len(include) > 0:\n",
    "        invertible, _ = is_invertible(Sigma[include, :][:, include], tol=tol)\n",
    "        if not invertible:\n",
    "            warnings.warn(\"The variables that have been requested to be included are colinear.\")\n",
    "            reject = False\n",
    "            return np.concatenate([include, not_include[:(k - len(incude))]]), reject\n",
    "    \n",
    "    no_initialization = (S_init is None)\n",
    "    if not no_initialization or k == 0 or k == 1:\n",
    "        num_inits = 1\n",
    "\n",
    "    for _ in range(num_inits):\n",
    "        if no_initialization:\n",
    "            S_init = np.concatenate([include, np.random.choice(not_include, k-len(include), replace=False)]).astype(int)\n",
    "\n",
    "        S, reject, log_det = swapping_subset_factor_with_init(Sigma=Sigma,\n",
    "                                                              S_init=S_init,\n",
    "                                                              find_minimizer=find_minimizer,\n",
    "                                                              cutoff=cutoff,\n",
    "                                                              max_iter=max_iter, \n",
    "                                                              include=include,\n",
    "                                                              exclude=exclude,\n",
    "                                                              tol=TOL)\n",
    "        if not find_minimizer and (not reject):\n",
    "            return S, reject \n",
    "        \n",
    "        if find_minimizer and (not reject):\n",
    "            reject = reject\n",
    "\n",
    "        if log_det < best_log_det:\n",
    "            best_S = S\n",
    "            best_log_det = log_det \n",
    "\n",
    "    return best_S, reject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "87ec7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_null_dist(n, p, k, B=int(1e5), seed=0):\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(0)\n",
    "\n",
    "    num_adjusted_samples = n - k - 1\n",
    "    num_features = p-k\n",
    "    full_dfs = np.array([num_adjusted_samples - i + 1 for i in range(1, num_features + 1)])\n",
    "    full_chi_sqs = np.random.chisquare(df=full_dfs, size=(B, len(full_dfs)))\n",
    "\n",
    "    null_dfs = np.arange(1, num_features)\n",
    "    null_chi_sqs = np.random.chisquare(df=null_dfs, size=(B, len(null_dfs)))\n",
    "    null_chi_sqs = np.hstack([np.zeros(B).reshape((B, 1)), null_chi_sqs])\n",
    "    return n*(np.sum( np.log(null_chi_sqs/full_chi_sqs + 1), axis=1))\n",
    "\n",
    "def Q(qs, n, p, k, B=int(1e5), seed=0):\n",
    "\n",
    "    return np.quantile(sample_null_dist(n, p, k, B=B, seed=seed), qs)\n",
    "\n",
    "def subset_selection(X, \n",
    "                     alpha, \n",
    "                     include=np.array([]), \n",
    "                     exclude=np.array([]), \n",
    "                     quantile_dict={}, \n",
    "                     B=int(1e5),\n",
    "                     max_iter=100,\n",
    "                     num_inits=1,\n",
    "                     tol=TOL):\n",
    "    n, p = X.shape\n",
    "    _, Sigma_hat = get_moments(X)\n",
    "    Sigma_hat = standardize_cov(Sigma_hat)\n",
    "    \n",
    "    crit_vals = np.array([Q(1-alpha, n, p, i, B=B) if (1 - alpha, n, p , i) not in quantile_dict.keys() else quantile_dict[( 1 - alpha, n, p , i)] for i in range(p + 1)])\n",
    "    cutoffs = crit_vals/n  + np.linalg.slogdet(Sigma_hat)[1]\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    S, reject = greedy_subset_factor_selection(Sigma_hat,\n",
    "                                               cutoffs,\n",
    "                                               include=include,\n",
    "                                               exclude=exclude,\n",
    "                                               tol=tol)\n",
    "    end = time.time()\n",
    "    \n",
    "    if len(S) <= 1:\n",
    "        return S \n",
    "    if reject:\n",
    "        warnings.warn(\"We can still reject the model with this S, but nothing more can be added.\")\n",
    "        return S\n",
    "\n",
    "    k = len(S)\n",
    "    while not reject:\n",
    "        k = k-1\n",
    "        S, reject = swapping_subset_factor_selection(Sigma_hat,\n",
    "                                                     k,\n",
    "                                                     cutoffs[k],\n",
    "                                                     max_iter=max_iter,\n",
    "                                                     num_inits=num_inits,\n",
    "                                                     include=include,\n",
    "                                                     exclude=exclude,\n",
    "                                                     tol=TOL)\n",
    "        if reject:\n",
    "            S, reject = swapping_subset_factor_selection(Sigma_hat,\n",
    "                                                         k+1,\n",
    "                                                         cutoffs[k+1],\n",
    "                                                         max_iter=max_iter,\n",
    "                                                         num_inits=num_inits,\n",
    "                                                         find_minimizer=True,\n",
    "                                                         include=include,\n",
    "                                                         exclude=exclude,\n",
    "                                                         tol=TOL)\n",
    "            return S \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "05dddb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = pd.read_csv(\"../data/BFI228.csv\").values[:, 1:].astype(int)\n",
    "n, p = X.shape\n",
    "_, Sigma_hat = get_moments(X)\n",
    "\n",
    "alpha = 0.1\n",
    "quantile_dict = {(1 - alpha, n, p, i,):  Q(1 -alpha, n, p, i,) for i in range(p + 1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "e08feff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'p' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[435], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[43msubset_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mquantile_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantile_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnum_inits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m37\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(end \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[433], line 38\u001b[0m, in \u001b[0;36msubset_selection\u001b[0;34m(X, alpha, include, exclude, quantile_dict, B, max_iter, num_inits, tol)\u001b[0m\n\u001b[1;32m     34\u001b[0m cutoffs \u001b[38;5;241m=\u001b[39m crit_vals\u001b[38;5;241m/\u001b[39mn  \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mslogdet(Sigma_hat)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     36\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 38\u001b[0m S, reject \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_subset_factor_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSigma_hat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcutoffs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(S) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[432], line 43\u001b[0m, in \u001b[0;36mgreedy_subset_factor_selection\u001b[0;34m(Sigma, cutoffs, include, exclude, tol)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# check if size-0 subset is sufficient \u001b[39;00m\n\u001b[1;32m     42\u001b[0m reject \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39mdiag(Sigma))) \u001b[38;5;241m>\u001b[39m cutoffs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m reject \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(include) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exclude) \u001b[38;5;241m==\u001b[39m \u001b[43mp\u001b[49m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([]), reject\n\u001b[1;32m     46\u001b[0m Sigma_R \u001b[38;5;241m=\u001b[39m Sigma\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'p' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "S = subset_selection(X, \n",
    "                 alpha=alpha,  \n",
    "                 quantile_dict=quantile_dict, \n",
    "                 num_inits=1,\n",
    "                 include=np.array([1]),\n",
    "                 exclude=np.array([14, 37]))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "102d8736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "0bd2c577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2, 31, 35,  3, 29, 43,  6, 19, 13, 15, 23, 20, 27, 12, 32, 22,\n",
       "       38, 25,  0, 41, 24])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "4fe56821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "42569f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  6  7 12 13 14 15 17 20 24 25 27 29 30 37 38 41 42 43]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "k=20\n",
    "S, reject = swapping_subset_factor_selection(Sigma_hat,\n",
    "                                 k,\n",
    "                                 cutoffs[k-1],\n",
    "                                 max_iter=100,\n",
    "                                 num_inits=100,  \n",
    "                                 tol=TOL)\n",
    "\n",
    "print(np.sort(S))\n",
    "print(reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a79509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "66ceb331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0]\n",
      " [1 1 0 0 0]\n",
      " [0 0 3 0 0]\n",
      " [0 0 0 4 0]\n",
      " [0 0 0 0 5]]\n"
     ]
    }
   ],
   "source": [
    "Sigma = np.diag(np.array([1, 1,  3, 4, 5]))\n",
    "Sigma[0, 1] = 1\n",
    "Sigma[1, 0] = 1\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e9022b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 0, 4]),\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 3., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]),\n",
       " array([0, 3, 2]),\n",
       " True)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swapping_css(Sigma,\n",
    "           k=3,\n",
    "           max_iter=100,\n",
    "           num_inits=5, \n",
    "           include=np.array([0]),\n",
    "           exclude=np.array([]),\n",
    "           tol=TOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5bf7f4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 4, 2]),\n",
       " array([[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]),\n",
       " array([4, 0, 3]),\n",
       " True)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swapping_css(Sigma,\n",
    "           k=3,\n",
    "           max_iter=100,\n",
    "           num_inits=1, \n",
    "           include=np.array([]),\n",
    "           exclude=np.array([]),\n",
    "           tol=TOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
